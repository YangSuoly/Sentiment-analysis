ã€AIè¾¾äººåˆ›é€ è¥ç¬¬äºŒæœŸã€‘æ–‡æœ¬æƒ…æ„Ÿåˆ†æï¼ˆåŸºäºé£æ¡¨)
---

# ä¸€ã€é¡¹ç›®èƒŒæ™¯ä»‹ç»

åœ¨æˆ‘å›½ç”µå­å•†åŠ¡é£å¿«å‘å±•çš„èƒŒæ™¯ä¸‹ï¼ŒåŸºæœ¬ä¸Šæ‰€æœ‰çš„ç”µå­å•†åŠ¡ç½‘ç«™éƒ½æ”¯æŒæ¶ˆè´¹è€…å¯¹äº§å“çš„ç›¸å…³å†…å®¹ï¼ˆå•†å“ã€æœåŠ¡ã€å–å®¶ï¼‰ç­‰è¿›è¡Œæ‰“åˆ†å’Œå‘è¡¨è¯„è®ºã€‚å®¢æˆ·å¯ä»¥é€šè¿‡ç½‘ç»œè¿›è¡Œæ²Ÿé€šå’Œäº¤æµï¼Œåœ¨ç½‘ç»œå¹³å°ä¸Šå‘å¸ƒå¤§é‡çš„ç•™è¨€å’Œè¯„è®ºï¼Œè¿™å·²ç»æˆä¸ºäº’è”ç½‘çš„ä¸€ç§æµè¡Œå½¢å¼ï¼Œè€Œè¿™ç§å½¢åŠ¿å¿…ç„¶ç»™äº’è”ç½‘å¸¦æ¥æµ·é‡çš„ä¿¡æ¯ã€‚

å¯¹äº**å–å®¶**æ¥è¯´ï¼Œå¯ä»¥ä»è¯„è®ºä¿¡æ¯ä¸­è·å–å®¢æˆ·çš„å®é™…éœ€æ±‚ï¼Œä»¥æ”¹å–„äº§å“å“è´¨ï¼Œæé«˜è‡ªèº«çš„ç«äº‰åŠ›ã€‚å¯¹äº**å®¢æˆ·**æ¥è¯´ï¼Œå¯ä»¥å€Ÿé‰´åˆ«äººçš„è´­ä¹°å†å²ä»¥åŠè¯„è®ºä¿¡æ¯ï¼Œæ›´å¥½çš„è¾…åŠ©è‡ªå·±åˆ¶å®šè´­ä¹°å†³ç­–ã€‚æ­¤å¤–ï¼Œå¯¹äºä¸€äº›æœªçŸ¥ä½“éªŒäº§å“ï¼Œå®¢æˆ·å¯ä»¥é€šè¿‡ç½‘ç»œæ¥è·å–äº§å“ä¿¡æ¯ï¼Œç‰¹åˆ«æ˜¯å¯¹ä¸€äº›æœªçŸ¥çš„ä½“éªŒäº§å“ï¼Œå®¢æˆ·ä¸ºäº†é™ä½è‡ªèº«çš„é£é™©æ›´åŠ å€¾å‘äºå¾—åˆ°å…¶ä»–å®¢æˆ·çš„æ„è§å’Œçœ‹æ³•ï¼Œè¿™äº›è¯„è®ºå¯¹æ½œåœ¨çš„ä¹°å®¶è€Œè¨€æ— ç–‘æ˜¯ä¸€ç¬”è´¢å¯Œï¼Œå¹¶ä»¥æ­¤ä½œä¸ºå†³ç­–çš„é‡è¦ä¾æ®ã€‚ å› æ­¤ï¼Œé€šè¿‡åˆ©ç”¨æ•°æ®æŒ–æ˜æŠ€æœ¯é’ˆå¯¹å®¢æˆ·çš„å¤§é‡è¯„è®ºè¿›è¡Œåˆ†æï¼Œå¯ä»¥æŒ–æ˜å‡ºè¿™äº›ä¿¡æ¯çš„ç‰¹å¾ï¼Œè€Œå¾—åˆ°çš„è¿™äº›ä¿¡æ¯æœ‰åˆ©äºç”Ÿäº§å•†æ”¹è¿›è‡ªèº«äº§å“å’Œæ”¹å–„ç›¸å…³çš„æœåŠ¡ï¼Œæé«˜å•†å®¶çš„æ ¸å¿ƒç«äº‰åŠ›ã€‚æœ¬é¡¹ç›®æ‰€é€‰æ‹©çš„Multi-Domain Sentiment Dataset æ•°æ®åº“åŒ…å«ä» Amazon.com è·å–çš„æ¥è‡ªè®¸å¤šäº§å“ç±»å‹ï¼ˆåŸŸï¼‰çš„äº§å“è¯„è®ºï¼Œå¸Œæœ›é€šè¿‡å€ŸåŠ©è¯¥æ•°æ®åº“å®ç°NLPè¯­è¨€å¤„ç†æŠ€æœ¯åœ¨å¯¹å•†å“è¯„è®ºè¿›è¡Œæƒ…æ„Ÿåˆ†æï¼Œäº†è§£æ–‡æœ¬æƒ…æ„Ÿåˆ†ç±»çš„åŸºæœ¬ç”¨æ³•ã€‚

# äºŒã€æ•°æ®ä»‹ç»

## 2.1 æ•°æ®é›†ç®€ä»‹

Multi-Domain Sentiment Dataset åŒ…å«ä» Amazon.com è·å–çš„æ¥è‡ªè®¸å¤šäº§å“ç±»å‹ï¼ˆåŸŸï¼‰çš„äº§å“è¯„è®ºã€‚æŸäº›åŸŸï¼ˆä¹¦ç±å’Œ DVDï¼‰æœ‰æ•°åä¸‡æ¡è¯„è®ºã€‚å…¶ä»–ï¼ˆä¹å™¨ï¼‰åªæœ‰å‡ ç™¾ã€‚è¯„è®ºåŒ…å«æ˜Ÿçº§è¯„åˆ†ï¼ˆ1 åˆ° 5 æ˜Ÿï¼‰ï¼Œå¦‚æœéœ€è¦ï¼Œå¯ä»¥å°†å…¶è½¬æ¢ä¸ºäºŒè¿›åˆ¶æ ‡ç­¾ã€‚æ­¤é¡µé¢åŒ…å«æœ‰å…³æ•°æ®çš„ä¸€äº›è¯´æ˜ã€‚

å…³äºæ•°æ®é›†çš„ä¸€äº›æ³¨æ„äº‹é¡¹:
1. unprocessed æ–‡ä»¶å¤¹åŒ…å«åŸå§‹æ•°æ®ã€‚
2. processed.acl æ–‡ä»¶å¤¹åŒ…å«é¢„å¤„ç†å’Œå¹³è¡¡çš„æ•°æ®ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼ŒBlitzer ç­‰äººçš„æ ¼å¼ã€‚(ACL 2007)
3. processed.realvalued åŒ…å«äº†ç»è¿‡é¢„å¤„ç†å’Œå¹³è¡¡çš„æ•°æ®ï¼Œä½†æ˜¯æœ‰æ˜Ÿæ•°ï¼Œè€Œä¸ä»…ä»…æ˜¯æ­£æ•°æˆ–è´Ÿæ•°ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼ŒMansour ç­‰äººçš„æ ¼å¼ã€‚(NIPS 2009)

    é¢„å¤„ç†æ•°æ®ä¸ºæ¯ä¸ªæ–‡æ¡£ä¸€è¡Œï¼Œæ¯è¡Œæ ¼å¼å¦‚ä¸‹ï¼š
    
    feature:<count> .... feature:<count> #label#:<label>
  
    æ ‡ç­¾æ€»æ˜¯åœ¨æ¯ä¸ªæ–‡ä»¶çš„æœ«å°¾çº¿ã€‚
4. æ¯ä¸ªç›®å½•å¯¹åº”ä¸€ä¸ªåŸŸã€‚æ¯ä¸ªç›®å½•åŒ…å«å‡ ä¸ªæ–‡ä»¶ï¼Œæˆ‘ä»¬ç®€è¦ä»‹ç»ä¸€ä¸‹ï¼š
    - all.review -- è¯¥åŸŸçš„æ‰€æœ‰è¯„è®ºï¼Œä»¥å…¶åŸå§‹æ ¼å¼
    - positive.review -- æ­£é¢è¯„è®º
    - negative.review -- è´Ÿé¢è¯„è®º
    - unlabeled.review -- æœªæ ‡è®°çš„è¯„è®º
    - processed.review -- é¢„å¤„ç†è¯„è®ºï¼ˆè§ä¸‹æ–‡ï¼‰
    - processing.review.balanced -- é¢„å¤„ç†è¯„è®ºï¼Œåœ¨æ­£é¢å’Œè´Ÿé¢ä¹‹é—´å‡è¡¡ã€‚

5. è™½ç„¶æ­£é¢å’Œè´Ÿé¢æ–‡ä»¶åŒ…å«æ­£é¢å’Œè´Ÿé¢è¯„è®ºï¼Œä½†è¿™äº›ä¸ä¸€å®šæ˜¯ä»»ä½•å¼•ç”¨è®ºæ–‡ä¸­ä½¿ç”¨çš„åˆ†å‰²ã€‚ä»–ä»¬åªæ˜¯åœ¨é‚£é‡Œå°½å¯èƒ½åœ°è¿›è¡Œåˆå§‹åˆ†è£‚ã€‚
6. æ¯ä¸ªï¼ˆæœªå¤„ç†çš„ï¼‰æ–‡ä»¶éƒ½åŒ…å«ä¸€ä¸ªç”¨äºå¯¹è¯„è®ºè¿›è¡Œç¼–ç çš„ä¼ª XML æ–¹æ¡ˆã€‚å¤§å¤šæ•°å­—æ®µéƒ½æ˜¯ä¸è¨€è‡ªæ˜çš„ã€‚è¯„è®ºæœ‰ä¸€ä¸ªä¸æ˜¯å¾ˆç‹¬ç‰¹çš„å”¯ä¸€ ID å­—æ®µã€‚å¦‚æœå®ƒæœ‰ä¸¤ä¸ªå”¯ä¸€çš„ id å­—æ®µï¼Œåˆ™å¿½ç•¥ä»…åŒ…å«ä¸€ä¸ªæ•°å­—çš„é‚£ä¸ªã€‚æ€»æœ‰ä¸€äº›æˆ‘ä»¬å¯èƒ½å¿½ç•¥çš„å°ç»†èŠ‚ã€‚å¦‚æœæ‚¨åœ¨é˜…è¯»è®ºæ–‡å’Œæœ¬é¡µåæœ‰ä»»ä½•ç–‘é—®ï¼Œè¯·å‘Šè¯‰ Mark Dredze æˆ– John Blitzerã€‚


```python
data_path = 'data/'
# all_path = data_path + 'all.review'
positive_path = data_path + 'positive.review'
negative_path = data_path + 'negative.review'
# unbalance_path = data_path + 'unbalanced.review'
# test_path = data_path + 'processed.review.balanced'
# processed_path = data_path + 'processed.review'


def read_dataset(dataset_path):
    # ========== å¯ä»¥é€šè¿‡ä¸‹è¿°ä»£ç æŸ¥çœ‹ txt æ–‡ä»¶çš„ç¼–ç 
    # import chardet
    # f = open(positive_path,'rb')
    # data = f.read()
    # print(chardet.detect(data))
    # =============================================

    with open(dataset_path, encoding = 'ISO-8859-1') as f:
        train = f.readlines()
    
    # è¿›è¡Œç®€å•çš„ç­›é€‰ï¼Œå°†å°äº 15 ä¸ªå•è¯çš„å¥å­è¿›è¡Œå‰”é™¤
    train = [i for i in train if not i.startswith('<')]
    train = [i for i in train if len(i.split(' ')) >= 15]

    return train
```

## 2.2 é€‰å–æ•°æ®é›†
    
è€ƒè™‘åˆ°æ•°æ®é›†æ•°æ®é‡è¾ƒå¤§ï¼Œæ­¤å¤„æˆ‘ä»¬è¿›é€‰æ‹©å…¶ä¸­çš„ä¸€å°éƒ¨åˆ†ï¼šprocessed_stars/electronics æ–‡ä»¶å¤¹ä¸­çš„æ•°æ®ä½œä¸ºæœ¬é¡¹ç›®çš„æ•°æ®è¿›è¡Œåˆ†æã€‚

- åŠ è½½æ•°æ® 

    é€šè¿‡åˆæ­¥æŸ¥çœ‹ï¼Œå‘ç°æ•°æ®é›†ä¸­æœ‰è®¸å¤šæ— æ•ˆçš„è¯„è®ºï¼Œå› æ­¤æ­¤å¤„è¿›è¡Œäº†ç®€å•ç­›é€‰ï¼Œå‰”é™¤å°äº `15` ä¸ªå•è¯çš„è¯„è®ºï¼Œä¸ºäº†ä¾¿åˆ©ï¼Œæ­¤é¡¹ç›®è¿›é˜Ÿ `positve` å’Œ `negative` è¿›è¡Œè¯†åˆ«å’Œåˆ†æã€‚


```python
positive_dataset = read_dataset(positive_path)
negative_dataset = read_dataset(negative_path)

print('The length of Positive dataset: ', len(positive_dataset))
print('The length of Negative dataset: ', len(negative_dataset))
```

    The length of Positive dataset:  2224
    The length of Negative dataset:  2428


å¯ä»¥å‘ç°ï¼Œå¾—åˆ°äº† `2224` æ¡ç§¯æè¯„è®ºï¼Œ`2428` æ¡è´Ÿå‘è¯„è®ºã€‚

### 2.2.2 æ–‡æœ¬å¤„ç†æ–¹æ³•

ç”±äºæœ¬é¡¹ç›®æ‰€é€‰æ•°æ®ä¸ºè‹±è¯­ï¼Œå› æ­¤é€‰ç”¨ `NLTK` åº“æ¥å¤„ç†æˆ‘ä»¬çš„è‹±è¯­æ–‡æœ¬æ•°æ®ï¼Œè€Œåœ¨å€ŸåŠ© `NLTK` å¤„ç†å¥å­æ—¶ï¼Œéœ€è¦åŠ è½½å…¶è‡ªå®šä¹‰çš„ `tokenizer`ï¼Œç”±äºæ— æ³•ç›´æ¥ä¸‹è½½ï¼Œæ­¤å¤„æˆ‘ä»¬é€‰æ‹©å°† `punkt.zip` ä¸Šä¼ è‡³ `/home/aistudio/nltk_data/tokenizers/` æ–‡ä»¶è·¯å¾„ï¼Œä»¥ä¿è¯ `NLTK` å¯ä»¥æ­£å¸¸å¤„ç†æ•°æ®ã€‚è¯¥å‹ç¼©åŒ…å¯ä»¥é€šè¿‡ç½‘ä¸ŠæŸ¥æ‰¾å¾—åˆ°ã€‚

    
    
é¦–å…ˆï¼Œä½¿ç”¨äº†è¯äº‘å¯¹è¯„è®ºè¿›è¡Œäº†å¯è§†åŒ–åˆ†æï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºï¼š
    
![](./Img/Picture2.png)
    


```python
!pip install wordcloud
```

    Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple
    Collecting wordcloud
      Downloading https://pypi.tuna.tsinghua.edu.cn/packages/1b/06/0516bdba2ebdc0d5bd476aa66f94666dd0ad6b9abda723fdf28e451db919/wordcloud-1.8.1-cp37-cp37m-manylinux1_x86_64.whl (366 kB)
         |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 366 kB 2.0 MB/s            
    [?25hRequirement already satisfied: numpy>=1.6.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from wordcloud) (1.19.5)
    Requirement already satisfied: matplotlib in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from wordcloud) (2.2.3)
    Requirement already satisfied: pillow in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from wordcloud) (8.2.0)
    Requirement already satisfied: python-dateutil>=2.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from matplotlib->wordcloud) (2.8.2)
    Requirement already satisfied: cycler>=0.10 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from matplotlib->wordcloud) (0.10.0)
    Requirement already satisfied: six>=1.10 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from matplotlib->wordcloud) (1.16.0)
    Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from matplotlib->wordcloud) (3.0.7)
    Requirement already satisfied: pytz in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from matplotlib->wordcloud) (2019.3)
    Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from matplotlib->wordcloud) (1.1.0)
    Requirement already satisfied: setuptools in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from kiwisolver>=1.0.1->matplotlib->wordcloud) (41.4.0)
    Installing collected packages: wordcloud
    Successfully installed wordcloud-1.8.1
    [33mWARNING: You are using pip version 21.3.1; however, version 22.0.3 is available.
    You should consider upgrading via the '/opt/conda/envs/python35-paddle120-env/bin/python -m pip install --upgrade pip' command.[0m



```python
from wordcloud import WordCloud 
import matplotlib.pyplot as plt 

results = {}

test_words = ' '.join(positive_dataset)

wordcloud = WordCloud(collocations = False, 
    background_color = 'white',
    width = 2800,
    height = 1600
    ).generate(test_words)

plt.figure()
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.show()
```


    
![png](./Img/output_10_0.png)
    


å¯ä»¥å‘ç°ï¼Œä¸å‡ºæ‰€æ–™ï¼Œäº§å“è¯„è®ºä¸­å‡ºç°æœ€å¤šçš„å‡ ä¸ªè¯åˆ†åˆ«ä¸ºï¼š`book`, `read`, `time`ã€‚
    
## 2.3 æ•°æ®é¢„å¤„ç†
    
1. æ•°æ®æ ‡ç­¾æ ‡æ³¨
    
åœ¨è¿›è¡Œåˆ†æä¹‹å‰ï¼Œéœ€è¦å¯¹æ•°æ®çš„æ ‡ç­¾è¿›è¡Œå¤„ç†ï¼Œé¦–å…ˆå¯¹ `positive` å’Œ `negative` æ ·æœ¬è¿›è¡Œæ ‡æ³¨ï¼Œæ ‡æ³¨è§„åˆ™ä¸ºï¼š
    
- positive: 1
- negative: 0
   


```python
import os 
import pandas as pd 

export_path = 'Senti_dataset/'
all_data_path = export_path + 'all.csv'
# æŠŠç”Ÿæˆçš„æ•°æ®åˆ—è¡¨éƒ½æ”¾åœ¨è‡ªå·±çš„æ€»ç±»åˆ«æ–‡ä»¶å¤¹ä¸­

if os.path.exists(all_data_path):
    all_pd = pd.read_csv(all_data_path, encoding = 'utf-8', index_col = 0)
else:
    pos_pd = pd.DataFrame(positive_dataset, columns = ['Sentences'])
    pos_pd['Label'] = 1 # Positive

    neg_pd = pd.DataFrame(negative_dataset, columns = ['Sentences'])
    neg_pd['Label'] = 0

    all_pd = pd.concat([pos_pd, neg_pd], axis = 0)
    all_pd = all_pd.sample(frac=1).reset_index(drop=True) # æ‰“æ•£æ•°æ®
    all_pd.to_csv(all_data_path, encoding = 'utf-8')
```

2. æ„å»ºæ•°æ®å­—å…¸
   


```python
import nltk 

def create_dic(df, dict_path):
    with open(dict_path, 'w') as f:
        f.seek(0)
        f.truncate()

    dict_set = set()
    
    contents = ' '.join(df['Sentences'].to_list())

    tokens = nltk.word_tokenize(contents)
    s = nltk.stem.SnowballStemmer('english')
    clean_text = [s.stem(ws) for ws in tokens]

    for s in clean_text:
        dict_set.add(s)

    dict_list = []
    i = 0
    for s in dict_set:
        dict_list.append([s, i])
        i += 1
    
    dict_txt = dict(dict_list)
    end_dict = {'<unk>': i, '<pad>': i+1}
    dict_txt.update(end_dict)

    with open(dict_path, 'w', encoding='utf-8') as f:
        f.write(str(dict_txt))
    
    print('The word dictionary has been created successfully!')

# dict_path = export_path + 'word2idx.txt'
# create_dic(all_pd, dict_path)
```

 
3. ç”Ÿæˆå•è¯è¡¨
    


```python
import os

def load_vocab(dict_path):
    if os.path.exists(dict_path) == False:
        create_dic(all_pd, dict_path)

    with open(dict_path, 'r', encoding = 'utf-8') as fr:
        vocab = eval(fr.read())
    return vocab

vocab = load_vocab(dict_path)
vocab
```




    {'synonym': 0,
     'juri': 1,
     'websit': 2,
     'sharp-tongu': 3,
     'frequent': 4,
     '..i': 5,
     'destroy': 6,
     'variabl': 7,
     'pipher': 8,
     'shawn': 9,
     'counter': 10,
     'shrift': 11,
     'haul': 12,
     'encyclopaed': 13,
     'papel': 14,
     'readng': 15,
     'simpl': 16,
     'cross-countri': 17,
     'crossword': 18,
     '8-12': 19,
     'salad': 20,
     'fast-pac': 21,
     'portal': 22,
     'curb': 23,
     'misstat': 24,
     'infedil': 25,
     'enamor': 26,
     'lens': 27,
     'chargeback': 28,
     'hippo': 29,
     'rylant': 30,
     'patho': 31,
     'sens': 32,
     'nois': 33,
     'gorbachev': 34,
     'dentistri': 35,
     'damor': 36,
     'likelihood': 37,
     'conspicu': 38,
     'hop': 39,
     'pittsburgh': 40,
     'kabbalah': 41,
     'diagram': 42,
     '2.there': 43,
     'slightest': 44,
     'elopement.': 45,
     '111': 46,
     'attest': 47,
     'document': 48,
     'stultifi': 49,
     'lower': 50,
     'says\x1a': 51,
     'fiance': 52,
     'grovel': 53,
     '40s': 54,
     'oldster': 55,
     'unreli': 56,
     'rwanda': 57,
     'thecultur': 58,
     '80,000': 59,
     'unfett': 60,
     'beep': 61,
     'bergey': 62,
     'state-sponsor': 63,
     'rosenth': 64,
     'torrijos.who': 65,
     '5': 66,
     'epilogu': 67,
     'embolden': 68,
     'palat': 69,
     'dreamlik': 70,
     'bioweapon': 71,
     'fleurberg': 72,
     'certain': 73,
     'grate': 74,
     'forest': 75,
     'certain\x1aconclus': 76,
     'many\x1aextrem': 77,
     'osteen': 78,
     'habsburg': 79,
     'collar': 80,
     'famish': 81,
     'priceless': 82,
     'unsustain': 83,
     '*think*': 84,
     'arm': 85,
     'normal\x1a': 86,
     'restitut': 87,
     'rabbin': 88,
     'unengag': 89,
     'mat': 90,
     'mrs': 91,
     'hitchcock': 92,
     'tyre': 93,
     'cumbersom': 94,
     'pure-literari': 95,
     'drive-bi': 96,
     'nine': 97,
     'over-emphas': 98,
     'diari': 99,
     'algorithm': 100,
     "o'otham": 101,
     'mathematician': 102,
     '7.8.05': 103,
     'teri': 104,
     'malkin': 105,
     'hanukkah': 106,
     'language\x1a': 107,
     'revis': 108,
     'kelp': 109,
     'hard-to-put-down': 110,
     'targ': 111,
     'burden': 112,
     'stellar': 113,
     'fact': 114,
     'greenblatt': 115,
     'all-se': 116,
     'hypothes': 117,
     '161': 118,
     'underw': 119,
     'branden': 120,
     'comments\x1a': 121,
     'mcgovern': 122,
     'partin': 123,
     'apologet': 124,
     'larkcom': 125,
     'lyn': 126,
     'lacklust': 127,
     'katschei': 128,
     'ostens': 129,
     'feather': 130,
     'surgeon': 131,
     'fixat': 132,
     'less-abl': 133,
     'academ': 134,
     'feud': 135,
     'analysi': 136,
     'infrar': 137,
     'retri': 138,
     '7/2005': 139,
     'damon': 140,
     'impli': 141,
     'manhatten': 142,
     'personal\x1a': 143,
     'out-of-luck': 144,
     'syllabl': 145,
     'pronoun': 146,
     'inquiri': 147,
     'shake': 148,
     'bart': 149,
     'brick': 150,
     'legal': 151,
     'marvel': 152,
     'irrepress': 153,
     'frasier': 154,
     'materialist': 155,
     'defac': 156,
     'lass': 157,
     'theory-ori': 158,
     '.if': 159,
     'wacki': 160,
     'interrupt': 161,
     'cult-lik': 162,
     'way': 163,
     'vinton': 164,
     'theall': 165,
     'vorkosigan': 166,
     'insuffer': 167,
     'hillari': 168,
     'miniseri': 169,
     'said': 170,
     'hole': 171,
     'pathetic-excuss': 172,
     'blue-collar': 173,
     'bumpi': 174,
     'lennox': 175,
     'prophesi': 176,
     'slant': 177,
     'marri': 178,
     'stifl': 179,
     'sweep': 180,
     'blade': 181,
     'join': 182,
     'ok.': 183,
     'untruth': 184,
     'understat': 185,
     'complic': 186,
     'staf': 187,
     'systemat': 188,
     'hamburg': 189,
     'neural': 190,
     'hypocrisi': 191,
     'statement': 192,
     'butter': 193,
     'dominick': 194,
     'pillow': 195,
     'mingl': 196,
     'swear': 197,
     'globals/ref': 198,
     'tyler': 199,
     'step-by-step': 200,
     'just': 201,
     'scarlett': 202,
     'arabia': 203,
     'player': 204,
     'varous': 205,
     'jew': 206,
     'histor': 207,
     'german-pomeranian': 208,
     'abras': 209,
     'demograph': 210,
     'pic': 211,
     'breviari': 212,
     'liftig': 213,
     'ineffect': 214,
     'virgil': 215,
     'would-b': 216,
     'amor': 217,
     'dallair': 218,
     'litani': 219,
     'slow-go': 220,
     'honor': 221,
     'round': 222,
     'bingo': 223,
     'mulch': 224,
     '-incred': 225,
     'reigns.\x1a\x1a': 226,
     'millar': 227,
     'structur': 228,
     'declar': 229,
     'antonio': 230,
     'scalabl': 231,
     'karloff': 232,
     '-10': 233,
     'suppress': 234,
     'yack': 235,
     'tablet': 236,
     'timespan': 237,
     'rodrick': 238,
     'buddha': 239,
     'bother': 240,
     'sigmund': 241,
     'scandl': 242,
     'decid': 243,
     '*btw': 244,
     'small-mid': 245,
     'saxoni': 246,
     'al-qaeda': 247,
     '1980': 248,
     'octavius': 249,
     'hannigan': 250,
     'weather': 251,
     'ettus': 252,
     'elena': 253,
     'hum': 254,
     'these': 255,
     'help': 256,
     'resouc': 257,
     '7': 258,
     'hogath': 259,
     'monti': 260,
     'alik': 261,
     'amend': 262,
     'lovabl': 263,
     'london': 264,
     'two-by-two': 265,
     'clad': 266,
     'incest': 267,
     'braugher': 268,
     'peril': 269,
     'choir': 270,
     'rather': 271,
     'insur': 272,
     'legitim': 273,
     'bolster': 274,
     'de': 275,
     'though': 276,
     'man-hat': 277,
     'unquestion': 278,
     'synopsi': 279,
     'geographi': 280,
     'betsy-bug': 281,
     'warm-fuzzi': 282,
     'hong': 283,
     'cassidi': 284,
     'inner': 285,
     '1930': 286,
     'patricia': 287,
     'perus': 288,
     'tabl': 289,
     'dhun': 290,
     'seri': 291,
     'attention-seek': 292,
     'traine': 293,
     'sandler': 294,
     'maneuv': 295,
     'yap': 296,
     'ocd': 297,
     'bushido': 298,
     'grind': 299,
     'headstrong': 300,
     'jamieson': 301,
     'ladi': 302,
     'even\x1a': 303,
     'fifth': 304,
     'eso': 305,
     'suspend': 306,
     '200': 307,
     'helmsley': 308,
     'prudent': 309,
     '1204': 310,
     'treati': 311,
     'tank': 312,
     'option': 313,
     'pervers': 314,
     'hearti': 315,
     'studi': 316,
     'support': 317,
     'mickey': 318,
     'mainten': 319,
     'baedek': 320,
     'punishing\x1a': 321,
     'niffenegg': 322,
     'soul-bear': 323,
     'nut': 324,
     'mr': 325,
     '1979': 326,
     'pest': 327,
     'hyperemesi': 328,
     'comic': 329,
     'schama': 330,
     'underlin': 331,
     'skrull': 332,
     'elud': 333,
     'genocid': 334,
     'acceler': 335,
     'facet': 336,
     'braden': 337,
     'craig': 338,
     'fade': 339,
     'raptur': 340,
     'gushi': 341,
     'mumbo': 342,
     'he/sh': 343,
     'lynn': 344,
     'leash': 345,
     'startl': 346,
     'illumnati': 347,
     'pp.129-131': 348,
     'nocturn': 349,
     '41': 350,
     'concern': 351,
     'compil': 352,
     'healthcar': 353,
     '*i': 354,
     'count-down': 355,
     'backward': 356,
     'katharin': 357,
     'neolith': 358,
     'lack': 359,
     'overboard': 360,
     'fascist': 361,
     'heavily-popul': 362,
     'gorgeous': 363,
     'stackpol': 364,
     'coherent\x1a': 365,
     'been': 366,
     'evidence-bas': 367,
     'calhoun': 368,
     'linux': 369,
     'decoupled\x1a': 370,
     'sandra': 371,
     'reviv': 372,
     'odd': 373,
     'allig': 374,
     'passchendael': 375,
     'itself': 376,
     'j-b': 377,
     'must-read': 378,
     'jehovah': 379,
     'three-hour': 380,
     'pulpit': 381,
     'peddl': 382,
     'social': 383,
     'peggi': 384,
     'shamblin': 385,
     'high-abl': 386,
     'outrag': 387,
     'reconstructionist': 388,
     'braga': 389,
     'cobb': 390,
     'motley': 391,
     'airtim': 392,
     'seminari': 393,
     'ignit': 394,
     'hyper': 395,
     'playmat': 396,
     'referenc': 397,
     'catechist': 398,
     'shi': 399,
     'audiobook': 400,
     'demystifi': 401,
     'pregnant': 402,
     'dude': 403,
     'phone': 404,
     'skandier': 405,
     'startup': 406,
     'agon': 407,
     'influenti': 408,
     'physiocrat': 409,
     'term': 410,
     'come-back': 411,
     'stude': 412,
     'deliveri': 413,
     'then': 414,
     'language-learn': 415,
     'isi': 416,
     'brochur': 417,
     'chemistri': 418,
     'ensur': 419,
     'choos': 420,
     'call': 421,
     'pain-fre': 422,
     'boyd': 423,
     'benjamin': 424,
     'ned': 425,
     'arab': 426,
     'are': 427,
     'tumor': 428,
     'detroit': 429,
     '1906': 430,
     'surpis': 431,
     'buck': 432,
     'favorit': 433,
     'reciev': 434,
     'diabol': 435,
     'tom': 436,
     'relentless': 437,
     'barzini': 438,
     'dimension': 439,
     'business/oil': 440,
     'pizarro': 441,
     'ted': 442,
     'ivanho': 443,
     'fullest': 444,
     'level': 445,
     'adjust': 446,
     'monotonon': 447,
     '260': 448,
     'she-ocraci': 449,
     'lisa': 450,
     'shakespear': 451,
     'doped-up': 452,
     'prioriti': 453,
     'lesser': 454,
     'dt': 455,
     'accent': 456,
     'technology-savvi': 457,
     'fact\x1a': 458,
     'whyte': 459,
     'vice-presidenti': 460,
     'unto': 461,
     'roust': 462,
     'beta': 463,
     'church': 464,
     'lunch': 465,
     'billiard': 466,
     'probabl': 467,
     'gm': 468,
     'urgent': 469,
     'would': 470,
     'pike': 471,
     'troubl': 472,
     'manouv': 473,
     'thirty-year': 474,
     'j.f.groot': 475,
     'one-man': 476,
     'lovesick': 477,
     'reimer': 478,
     'letter': 479,
     'drawn': 480,
     'yore': 481,
     'spade': 482,
     'laissez-fair': 483,
     'convic': 484,
     'per': 485,
     'camus': 486,
     '.although': 487,
     'of\x1a': 488,
     'fraction': 489,
     'moneybal': 490,
     'unveil': 491,
     'al': 492,
     'scjp': 493,
     'sjoholm': 494,
     'hewlett-packard': 495,
     'happi': 496,
     'painstak': 497,
     'completist': 498,
     'jounrey': 499,
     'bryant': 500,
     'trite': 501,
     'shaddow': 502,
     '800+': 503,
     'brownsvill': 504,
     'simple-mind': 505,
     'hope': 506,
     'titur': 507,
     'improv': 508,
     'tunnel': 509,
     'verbatim': 510,
     'goldschneid': 511,
     'hid': 512,
     'beach': 513,
     'winger': 514,
     'geopolit': 515,
     'hyster': 516,
     'somoon': 517,
     'absurd': 518,
     'underdog': 519,
     'rose-color': 520,
     'sicken': 521,
     'gw': 522,
     'idea': 523,
     'make': 524,
     'menstruat': 525,
     'moon': 526,
     'hanniti': 527,
     '..but': 528,
     'murder': 529,
     'osborn': 530,
     'agnost': 531,
     'deduct': 532,
     'hes': 533,
     'laini': 534,
     '310-025': 535,
     'determin': 536,
     'jfk': 537,
     'tennant': 538,
     'pp.271-273': 539,
     'bufe': 540,
     'takeov': 541,
     'gal': 542,
     'common-sens': 543,
     'bad-guy': 544,
     "gen-x'er": 545,
     'operating\x1a': 546,
     'free': 547,
     'dungeon': 548,
     'abl': 549,
     'rice': 550,
     '19th': 551,
     'clumsi': 552,
     'clairvoy': 553,
     'authent': 554,
     'supplement': 555,
     'cattl': 556,
     'purest': 557,
     'tilli': 558,
     'powerpl': 559,
     'portug': 560,
     'technolog': 561,
     'evening/': 562,
     'pois': 563,
     'browni': 564,
     'wall': 565,
     'enthusiast': 566,
     'emphas': 567,
     'preston/child': 568,
     'extinct': 569,
     'single\x1a': 570,
     'busier': 571,
     'hienlein': 572,
     'pragu': 573,
     'fuhnman': 574,
     'weight': 575,
     '*and*': 576,
     'polyglot': 577,
     'provac': 578,
     'extra': 579,
     'percent': 580,
     'connelli': 581,
     'supermodel': 582,
     'prom': 583,
     'reassur': 584,
     'subsum': 585,
     'coke': 586,
     'underli': 587,
     'thick-pil': 588,
     'overstat': 589,
     '-clich': 590,
     'student': 591,
     'sumatra': 592,
     'presidenti': 593,
     'wolev': 594,
     'gahhhhhhh': 595,
     'ethiopia': 596,
     'hall': 597,
     'rumpel': 598,
     'lose': 599,
     'helena': 600,
     'belong': 601,
     'alarm': 602,
     'ver': 603,
     'interesting.a': 604,
     'insepar': 605,
     'generat': 606,
     'presenc': 607,
     'integ': 608,
     'es': 609,
     'fresco': 610,
     'pennsylvania': 611,
     'looks/sex': 612,
     'rinzler': 613,
     '77': 614,
     'consult': 615,
     'gelbspan': 616,
     'karl': 617,
     'hieroglyph': 618,
     'may': 619,
     'thwart': 620,
     'cheapest': 621,
     'liberalization.i': 622,
     'mid-80': 623,
     'elector': 624,
     'australia': 625,
     'outwit': 626,
     'nutrituion': 627,
     'laughabl': 628,
     'unsuccess': 629,
     'taker': 630,
     'mess': 631,
     'exual': 632,
     'wilso': 633,
     'shortcut': 634,
     'devious': 635,
     'overturn': 636,
     'shaq': 637,
     'semant': 638,
     'ricardo.in': 639,
     'pre-fiv': 640,
     'wetback': 641,
     'intuitive\x1a': 642,
     'wage': 643,
     'nicholaou': 644,
     'evolutionist': 645,
     'mx': 646,
     'start-up': 647,
     'foriegn': 648,
     'flesh': 649,
     'horc': 650,
     'mccarri': 651,
     'facin': 652,
     'ramaswami': 653,
     'kee': 654,
     'milk-cow': 655,
     'get.i': 656,
     'gerd': 657,
     'redempt': 658,
     'illumin': 659,
     'reprocess': 660,
     'run': 661,
     'conquest': 662,
     'liquor': 663,
     'self-publish': 664,
     'consciousness\x1a': 665,
     'extrasensori': 666,
     'pimpl': 667,
     'close.a': 668,
     'pardon': 669,
     'capit': 670,
     'book.it': 671,
     'catagori': 672,
     'beach-bag': 673,
     'travelogu': 674,
     'poison': 675,
     'despair': 676,
     'extent': 677,
     '222': 678,
     '-charli': 679,
     '.': 680,
     'rawlin': 681,
     'porti': 682,
     'poser': 683,
     'slipperi': 684,
     'occasion': 685,
     'bf': 686,
     'awak': 687,
     'victori': 688,
     'lon': 689,
     'shift': 690,
     'motorcycl': 691,
     'umpir': 692,
     'toe': 693,
     'deconstruction': 694,
     'colin': 695,
     'vegan': 696,
     'explan': 697,
     'marcus': 698,
     'farm-boy': 699,
     'aggreement': 700,
     'metali': 701,
     'acomplish': 702,
     'exceed': 703,
     'expound': 704,
     'dian': 705,
     'quad': 706,
     'armstrong': 707,
     'graceland': 708,
     'ot': 709,
     'csla': 710,
     'cardoza': 711,
     'put': 712,
     'mercuri': 713,
     'sex': 714,
     'entropi': 715,
     'festiv': 716,
     'deep': 717,
     'urbanist': 718,
     'booker': 719,
     'forgotten': 720,
     'drink': 721,
     'splendid': 722,
     'sketchi': 723,
     'sy': 724,
     'hippi': 725,
     'spur': 726,
     'neeli': 727,
     'hurri': 728,
     'especi': 729,
     'chemical/biolog': 730,
     'natali': 731,
     'israel': 732,
     'mouth': 733,
     'first-person': 734,
     'hamilton': 735,
     'bah': 736,
     'rip': 737,
     'triumvir': 738,
     'monoton': 739,
     'coauthor': 740,
     'eye': 741,
     'godfath': 742,
     'spielberg': 743,
     'anastassato': 744,
     'jk': 745,
     'unmask': 746,
     'wmds': 747,
     'nauseat': 748,
     'wilson': 749,
     'resarch': 750,
     'kara': 751,
     'fogarti': 752,
     'moveabl': 753,
     '178': 754,
     'demura': 755,
     'electrophil': 756,
     'conjectur': 757,
     'lewi': 758,
     'high-frequ': 759,
     'ninth-grad': 760,
     'jenkin': 761,
     'barbara': 762,
     'well-plac': 763,
     'gri': 764,
     'celin': 765,
     'intercom': 766,
     'pose': 767,
     'post-cold': 768,
     'unwind': 769,
     'subsequ': 770,
     'giuliani': 771,
     'sanitari': 772,
     'fought': 773,
     'be\x1a': 774,
     'anti-abort': 775,
     'shun': 776,
     'dive': 777,
     'vehement': 778,
     'throught': 779,
     'doe': 780,
     '224': 781,
     'taint': 782,
     'barad': 783,
     'bach': 784,
     'dip': 785,
     'lifer': 786,
     'subtleti': 787,
     'agaaiin': 788,
     'spectrum': 789,
     'personn': 790,
     'funda': 791,
     'team': 792,
     'comic-book': 793,
     'nucor': 794,
     'blaze': 795,
     'prosecut': 796,
     'mis-market': 797,
     'letdown': 798,
     'shadi': 799,
     'committe': 800,
     'anal': 801,
     'union': 802,
     'lonesom': 803,
     'ledford': 804,
     'romeo': 805,
     'rune': 806,
     'estimate.': 807,
     'leido': 808,
     'smoki': 809,
     'equat': 810,
     'ridden': 811,
     'imedi': 812,
     'crichton': 813,
     'grow': 814,
     'utilitarian': 815,
     'gape': 816,
     'papp': 817,
     'vbscript': 818,
     'foregoon': 819,
     'acn': 820,
     'the\x1agreeks.\x1a\x1abut': 821,
     'warmth': 822,
     'pam': 823,
     'self-conci': 824,
     'nobodi': 825,
     'xanadu': 826,
     'bound': 827,
     'feverish': 828,
     'questions.\x1a': 829,
     'ration': 830,
     'pomo': 831,
     'fleet': 832,
     'complex\x1a': 833,
     'deepend': 834,
     'vehicl': 835,
     'energeia': 836,
     'relationhsip': 837,
     'fortress': 838,
     'area': 839,
     'drozdeck': 840,
     'cheesi': 841,
     'glucos': 842,
     'sleazi': 843,
     'amish': 844,
     'inventori': 845,
     'bovari': 846,
     'hg': 847,
     '30': 848,
     'half-heart': 849,
     'grasshopp': 850,
     'abel': 851,
     'is\x1a': 852,
     'ancreon': 853,
     'sneedronningen': 854,
     'sheldon': 855,
     'music': 856,
     'namecheck': 857,
     'pre-trib': 858,
     'transport': 859,
     'scrupul': 860,
     'thirteen': 861,
     'allen': 862,
     'prey': 863,
     'orbit': 864,
     'penalti': 865,
     '50/50': 866,
     'self-coach': 867,
     'and..': 868,
     'dismal': 869,
     'picture-book': 870,
     'nmr': 871,
     'chemist': 872,
     '30-minut': 873,
     'tempor': 874,
     'minatur': 875,
     'audio': 876,
     'mean-spirit': 877,
     'thoughout': 878,
     'recours': 879,
     'framework': 880,
     'fallen': 881,
     'grasp\x1athi': 882,
     'eithy-f': 883,
     'tri': 884,
     'hahaha': 885,
     'add': 886,
     'eckhart': 887,
     'counterbal': 888,
     'splinter': 889,
     'journal': 890,
     'unexamin': 891,
     'stagnant': 892,
     'oligarch': 893,
     'paulin': 894,
     'shakespearean': 895,
     'unsuit': 896,
     'minutia': 897,
     'advanc': 898,
     'jw': 899,
     'smack': 900,
     'limit': 901,
     'soy-brais': 902,
     'luckiest': 903,
     'de-emphas': 904,
     'exclud': 905,
     'littlest': 906,
     'watchmen': 907,
     'tuckman': 908,
     'rau': 909,
     'haymarket': 910,
     'explicit': 911,
     'jeffers/ehrlich': 912,
     'mabey': 913,
     'recorvit': 914,
     'enthusisat': 915,
     'welfar': 916,
     'harsh': 917,
     '1994': 918,
     'lantern': 919,
     'middl': 920,
     'mentor': 921,
     'grievanc': 922,
     'clown': 923,
     'myth': 924,
     'custom': 925,
     'farfetch': 926,
     'fee': 927,
     'texbook': 928,
     'fairy-tal': 929,
     'collabor': 930,
     'mathison': 931,
     'easier': 932,
     'clout': 933,
     'queu': 934,
     'mole': 935,
     'logician': 936,
     'gravidarum': 937,
     'colleciton': 938,
     "bubba's/jacques/un": 939,
     'eleanor': 940,
     'husband': 941,
     'vie': 942,
     'launched/restart': 943,
     'zahl': 944,
     'twit': 945,
     'mogen': 946,
     'juxtapos': 947,
     'galett': 948,
     'amd': 949,
     'belt': 950,
     'hafez': 951,
     'darci': 952,
     'badon': 953,
     '319': 954,
     '2/3': 955,
     'gaze': 956,
     'upgrad': 957,
     'heroism': 958,
     'nich': 959,
     'hierarch': 960,
     'mail': 961,
     'mapmak': 962,
     'suk': 963,
     'overact': 964,
     'self-control': 965,
     'serf-lik': 966,
     '.after': 967,
     'gnaw': 968,
     'dullest': 969,
     'j.i': 970,
     'prehistor': 971,
     'warforg': 972,
     'lt': 973,
     'detract': 974,
     'wife': 975,
     'aspirin': 976,
     'nation': 977,
     'choppi': 978,
     'probe': 979,
     'shipwreck': 980,
     'connoisseur': 981,
     'burn': 982,
     'mileag': 983,
     'parcel': 984,
     'byrnison': 985,
     'includ': 986,
     'by\x1a': 987,
     'illiter': 988,
     'writer/read': 989,
     'andersen': 990,
     'use': 991,
     'cite': 992,
     'soften': 993,
     'loeb': 994,
     'unev': 995,
     'b/c': 996,
     'clare': 997,
     'neuharth': 998,
     'appar': 999,
     ...}



4. æ‰“æ•£æ•°æ®ï¼Œå¹¶å°†æ•°æ®é›†åˆ’åˆ†ä¸ºè®­ç»ƒé›†å’ŒéªŒè¯é›†
   


```python
def f_write_txt(sentences, word2idx, label):
    labs = ""
    ste = nltk.stem.SnowballStemmer('english')
    sentences = sentences.split(' ')
    for s in sentences:
        word = ste.stem(s)
        lab = str(word2idx.get(word, word2idx['<unk>']))
        # if word in word2idx.keys():
        #     lab = str(word2idx[word])
        # else:
        #     lab = str(word2idx['<unk>'])
        labs = labs + lab + ','
    labs = labs[:-1]
    labs = labs + '\t' + label + '\n'

    return labs 

def create_data_list(df, train_path, test_path, dict_path):
    dict_txt = load_vocab(dict_path)

    i = 0
    maxlen = 0
    with open(test_path, 'a', encoding='utf-8') as f_eval,open(train_path, 'a', encoding='utf-8') as f_train:
        for i, sentence in enumerate(df['Sentences'].values):
            label = str(df.loc[i, 'Label'])
            maxlen = max(maxlen, len(sentence.split(' ')))
            labs = f_write_txt(sentence, dict_txt, label)
            if i % 8 == 0:
                f_eval.write(labs)
            else:
                f_train.write(labs)
    print("æ•°æ®åˆ—è¡¨ç”Ÿæˆå®Œæˆï¼")
    print(maxlen)


# æŠŠç”Ÿæˆçš„æ•°æ®åˆ—è¡¨éƒ½æ”¾åœ¨è‡ªå·±çš„æ€»ç±»åˆ«æ–‡ä»¶å¤¹ä¸­
train_path = export_path + 'train_list.txt'
test_path = export_path + 'eval_list.txt'
dict_path = export_path + 'word2idx.txt'

#åœ¨ç”Ÿæˆæ•°æ®ä¹‹å‰ï¼Œé¦–å…ˆå°†eval_list.txtå’Œtrain_list.txtæ¸…ç©º
with open(test_path, 'w', encoding='utf-8') as f_eval:
    f_eval.seek(0)
    f_eval.truncate()
with open(train_path, 'w', encoding='utf-8') as f_train:
    f_train.seek(0)
    f_train.truncate() 

create_data_list(all_pd, train_path, test_path, dict_path)
```

    æ•°æ®åˆ—è¡¨ç”Ÿæˆå®Œæˆï¼
    877


 
ç»è¿‡ä¸Šè¿°å¤„ç†å®Œä¹‹åï¼Œæˆ‘ä»¬å·²ç»å°†éç»“æ„åŒ–çš„æ–‡æœ¬æ•°æ®è½¬åŒ–ä¸ºç»“æ„åŒ–çš„æ•°å€¼å‹æ•°æ®ï¼Œå¦‚ä¸‹:
    
![](./Img/Picture3.png)


```python
word2id = load_vocab(dict_path)

def ids_to_str(ids):
    words = []
    for k in ids:
        w = list(vocab.keys())[list(vocab.values()).index(int(k))]
        words.append(w if isinstance(w, str) else w.decode('ASCII'))
    return " ".join(words)

with open (test_path, 'r', encoding = 'utf-8') as f:
    i = 0
    lines = f.readlines()
    for line in lines:
        i += 1
        cols = line.strip().split("\t")
        if len(cols) != 2:
            sys.stderr.write("[NOTICE] Error Format Line!")
            continue
        
        label = int(cols[1])
        sentence = cols[0].split(',')
        print(str(i) + ":")
        print('sentence list id is:', sentence)
        print('sentence list is: ', ids_to_str(sentence))
        print('sentence label id is:', label)
        print('---------------------------------')
            
        if i == 2: break
```

    1:
    sentence list id is: ['13445', '1740', '2129', '17303', '17303', '13788', '13784', '3274', '5422', '17303', '12000', '2616', '6605', '8215', '4943', '17303', '1413', '3407', '5752', '16079', '7287', '524', '5412', '17303', '8215', '7287', '13445', '8215', '8438', '376', '3407', '16873', '15771', '17303']
    sentence list is:  in the end <unk> <unk> settl ani agrument for <unk> but rais new and interest <unk> it is a book that make you <unk> and that in and of itself is worth four <unk>
    sentence label id is: 1
    ---------------------------------
    2:
    sentence list id is: ['17303', '9593', '7357', '3461', '427', '15395', '7578', '1935', '17303', '17303', '9673', '5752', '3450', '8438', '13566', '6181', '7578', '991', '207', '10467', '5422', '1413', '17303', '1740', '2551', '6339', '3926', '8851', '13567', '4823', '8215', '524', '7714', '1740', '207', '114', '427', '8762', '17303', '17303']
    sentence list is:  <unk> i realiz they are suppos to be <unk> <unk> when a work of fiction attempt to use histor background for it <unk> the author should first do his homework and make sure the histor fact are complet <unk> <unk>
    sentence label id is: 0
    ---------------------------------


5. åˆ›å»º dataloader


```python
import paddle 
import numpy as np

from paddle.io import Dataset, DataLoader
from paddlenlp.datasets import MapDataset 

class MyDataset(Dataset):
    def __init__(self, dataset_path, vocab):
        self.dataset_path = dataset_path 
        self.vocab = vocab
        self.all_data = []

        with open (dataset_path, 'r', encoding = 'utf-8') as f:
            i = 0
            lines = f.readlines()
            for line in lines:
                i += 1
                cols = line.strip().split("\t")
                if len(cols) != 2:
                    sys.stderr.write("[NOTICE] Error Format Line!")
                    continue
                
                label = int(cols[1])
                sentence = cols[0].split(',')

                if len(sentence) >= 150:
                    sentence = np.array(sentence[:150]).astype('int64')   
                else:
                    sentence = np.concatenate([sentence, [vocab["<pad>"]]*(150-len(sentence))]).astype('int64')
                label = np.array(label)
                labels = np.array([0, 0]).astype('int64')
                labels[label] = 1
                self.all_data.append((sentence, labels))

    def __getitem__(self, idx):
        data, labels = self.all_data[idx]
        return data, labels

    def __len__(self):
        return len(self.all_data)

vocab = load_vocab(dict_path)
batch_size = 32
train_dataset = MyDataset(train_path, vocab)
test_dataset = MyDataset(test_path, vocab)

train_loader = DataLoader(train_dataset, places=paddle.CPUPlace(), return_list=True,
                                    shuffle=True, batch_size=batch_size, drop_last=True)
test_loader = DataLoader(test_dataset, places=paddle.CPUPlace(), return_list=True,
                                    shuffle=True, batch_size=batch_size, drop_last=True)

## check up
def ids_to_str(ids):
    words = []
    for k in ids:
        w = list(vocab.keys())[list(vocab.values()).index(int(k))]
        words.append(w if isinstance(w, str) else w.decode('ASCII'))
    return " ".join(words)

dict_sent = {1: 'Positive', 0: 'Negative'}

print('=============train_dataset =============') 
for data, label in train_dataset:
    print('Sentence list: ', data)
    print('Sentence (stem): ', ids_to_str(data))
    print('Shape: ', np.array(data).shape)
    print('Label: ', dict_sent[np.where(label==np.max(label))[0].item()])
    break


print('=============test_dataset =============') 
for data, label in test_dataset:
    print('Sentence list: ', data)
    print('Sentence (stem): ', ids_to_str(data))
    print('Shape: ', np.array(data).shape)
    print('Label: ', dict_sent[np.where(label==np.max(label))[0].item()])
    break
```

    =============train_dataset =============
    Sentence list:  [17303 17303   802  8438 11778  8215  5845  8438 14473 17063 12135  9859
      5839 12972 17303  8215  9546  7578  8851 16932  5098 17303 17303 17303
     17304 17304 17304 17304 17304 17304 17304 17304 17304 17304 17304 17304
     17304 17304 17304 17304 17304 17304 17304 17304 17304 17304 17304 17304
     17304 17304 17304 17304 17304 17304 17304 17304 17304 17304 17304 17304
     17304 17304 17304 17304 17304 17304 17304 17304 17304 17304 17304 17304
     17304 17304 17304 17304 17304 17304 17304 17304 17304 17304 17304 17304
     17304 17304 17304 17304 17304 17304 17304 17304 17304 17304 17304 17304
     17304 17304 17304 17304 17304 17304 17304 17304 17304 17304 17304 17304
     17304 17304 17304 17304 17304 17304 17304 17304 17304 17304 17304 17304
     17304 17304 17304 17304 17304 17304 17304 17304 17304 17304 17304 17304
     17304 17304 17304 17304 17304 17304 17304 17304 17304 17304 17304 17304
     17304 17304 17304 17304 17304 17304]
    Sentence (stem):  <unk> <unk> union of ideal and love of power has led men astray throughout <unk> and continu to do so even <unk> <unk> <unk> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>
    Shape:  (150,)
    Label:  Positive
    =============test_dataset =============
    Sentence list:  [13445  1740  2129 17303 17303 13788 13784  3274  5422 17303 12000  2616
      6605  8215  4943 17303  1413  3407  5752 16079  7287   524  5412 17303
      8215  7287 13445  8215  8438   376  3407 16873 15771 17303 17304 17304
     17304 17304 17304 17304 17304 17304 17304 17304 17304 17304 17304 17304
     17304 17304 17304 17304 17304 17304 17304 17304 17304 17304 17304 17304
     17304 17304 17304 17304 17304 17304 17304 17304 17304 17304 17304 17304
     17304 17304 17304 17304 17304 17304 17304 17304 17304 17304 17304 17304
     17304 17304 17304 17304 17304 17304 17304 17304 17304 17304 17304 17304
     17304 17304 17304 17304 17304 17304 17304 17304 17304 17304 17304 17304
     17304 17304 17304 17304 17304 17304 17304 17304 17304 17304 17304 17304
     17304 17304 17304 17304 17304 17304 17304 17304 17304 17304 17304 17304
     17304 17304 17304 17304 17304 17304 17304 17304 17304 17304 17304 17304
     17304 17304 17304 17304 17304 17304]
    Sentence (stem):  in the end <unk> <unk> settl ani agrument for <unk> but rais new and interest <unk> it is a book that make you <unk> and that in and of itself is worth four <unk> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>
    Shape:  (150,)
    Label:  Positive


# ä¸‰ã€CNNå·ç§¯ç¥ç»ç½‘ç»œæ¨¡å‹ä»‹ç»

## 3.1 æ¨¡å‹ç®€ä»‹

é‰´äº CNN ä¼˜ç§€çš„ç‰¹å¾æå–èƒ½åŠ›ï¼Œæœ¬é¡¹ç›®å°†å€ŸåŠ©å·ç§¯ç¥ç»ç½‘ç»œï¼ˆConvolutional neural network, CNNï¼‰æ¥æå–äº§å“è¯„è®ºä¸­ä¼ è¾¾çš„æƒ…æ„Ÿä¿¡æ¯ã€‚CNN ç¥ç»ç½‘ç»œä¸­å«æœ‰å·ç§¯æ»¤æ³¢å™¨ï¼Œèƒ½å¤Ÿç”¨äºå±€éƒ¨ç‰¹å¾çš„æå–ï¼Œå…¶è®¾è®¡çš„åˆè¡·æ˜¯ç”¨äºè®¡ç®—æœºè§†è§‰ï¼Œåæ¥è¢«å­¦è€…ä»¬å‘ç°åœ¨æ–‡æœ¬åˆ†ç±»ä¸­åŒæ ·èƒ½å–å¾—å‡ºè‰²çš„è¡¨ç°ã€‚CNN åˆ†ç±»å™¨çš„ä¸»è¦ç»“æ„æ˜¯å¤šå±‚ç½‘ç»œå’Œå·ç§¯æ¶æ„ï¼Œå…¶å·ç§¯å±‚ä¸»è¦è´Ÿè´£å¯¹è¾“å…¥çš„çŸ©é˜µè¿›è¡Œå·ç§¯è¿ç®—ï¼Œå› æ­¤å¯ä»¥ç»“åˆä¸åŒçš„è¯­ä¹‰ç‰‡æ®µç”¨äºå­¦ä¹ ç‰‡æ®µä¹‹é—´çš„ç›¸äº’å…³ç³»ã€‚

CNN çš„å¤„ç†é€»è¾‘å¦‚ä¸‹å›¾æ‰€ç¤ºã€‚é¦–å…ˆï¼Œåœ¨æ‰§è¡Œå·ç§¯æ“ä½œä¹‹å‰ï¼Œæ¯ä¸ªå•è¯ä¼šé€šè¿‡è¯åµŒå…¥çš„å½¢å¼è½¬åŒ–ä¸ºä¸€ä¸ªè¯å‘é‡è¡¨ç¤ºã€‚ç»è¿‡æ–‡æœ¬å‘é‡åŒ–ä¹‹åï¼Œæ¯ä¸€ä¸ªå¥å­éƒ½æ¢è½¬åŒ–ä¸ºä¸€ä¸ªå‘é‡çŸ©é˜µï¼ŒçŸ©é˜µçš„æ¯ä¸€è¡Œæ˜¯å•ä¸ªè¯å‘é‡è¡¨å¾ã€‚ç”±è¯å‘é‡æ„æˆçš„çŸ©é˜µå¯ä»¥è¢«è§†ä¸ºä¸€ä¸ªâ€œå›¾åƒâ€ï¼Œå› æ­¤å¯ä»¥å€ŸåŠ©çº¿æ€§è¿‡æ»¤å™¨ï¼ˆLinear filtersï¼‰è¿›è¡Œå·ç§¯æ“ä½œã€‚å¥å­é•¿åº¦å’Œè¿‡æ»¤å™¨çš„æ•°é‡å†³å®šäº†ç‰¹å¾å›¾çš„ç»´åº¦ï¼Œè¿‡æ»¤å™¨å¤§å°å†³å®šäº†æ¯æ¬¡å·ç§¯æ“ä½œä¸­åˆ’åˆ†çš„å•è¯æ•°é‡ã€‚

![](./Img/Picture1.png)

## 3.2 å‚æ•°è¯´æ˜

å¦‚ä¸Šå›¾ï¼Œæœ¬é¡¹ç›®é€‰æ‹©äº†ä¸€ä¸ªç®€å•çš„ CNN é¡¹ç›®è¿›è¡Œæ–‡æœ¬æ•°æ®çš„æƒ…æ„Ÿåˆ†æï¼Œé€‰å–çš„è¿‡æ»¤å™¨æ•°é‡ä¸º 100ï¼Œè¿‡æ»¤å™¨å¤§å°ï¼ˆFilter sizeï¼‰ä¸º 3ï¼Œè¿™æ„å‘³ç€æ¯æ¬¡å·ç§¯æ“ä½œä¼šç”± 200 ä¸ªå·ç§¯æ ¸å°†å•è¯åˆ’åˆ†ä¸¤ä¸ªæˆ–ä¸‰ä¸ªå•è¯ã€‚ç„¶åå¯¹å·ç§¯æ“ä½œçš„ç»“æœè¿›è¡Œæœ€å¤§å€¼æ± åŒ–é‡‡æ ·æ“ä½œï¼ˆ1-max poolingï¼‰æ¥ç”Ÿæˆå›ºå®šé•¿åº¦çš„å‘é‡ï¼Œè¯¥ç­–ç•¥æ˜¯ä»ç‰¹å¾å›¾ä¸­æå–ä¿¡æ¯çš„å¸¸ç”¨ç­–ç•¥ã€‚æœ€ç»ˆï¼Œå°†è¿‡æ»¤å™¨çš„æ˜ å°„ç»“æœè¿›è¡Œæ‹¼æ¥ç”Ÿæˆâ€œtop-levelâ€ç‰¹å¾å‘é‡ï¼Œéšåå€ŸåŠ© Softmax æ¿€æ´»å‡½æ•°å’Œ Dropout å±‚å¤„ç†è¯¥å‘é‡åç”Ÿæˆæœ€ç»ˆçš„äºŒåˆ†ç±»ç»“æœ ã€‚


```python
from paddle import nn, optimizer, metric
import paddle.nn.functional as F

export_path = 'Senti_dataset/'
all_data_path = export_path + 'all.csv'
# æŠŠç”Ÿæˆçš„æ•°æ®åˆ—è¡¨éƒ½æ”¾åœ¨è‡ªå·±çš„æ€»ç±»åˆ«æ–‡ä»¶å¤¹ä¸­
train_path = export_path + 'train_list.txt'
test_path = export_path + 'eval_list.txt'
dict_path = export_path + 'word2idx.txt'

EMBEDDING_DIM = 128
N_FILTER = 100
FILTER_SIZE = 3
CLASS_DIM = 2
NUM_CHANNELS = 1
BATCH_SIZE = 32
SEQ_LEN = 150
LEARNING_RATE = 0.002
```

- æ¨¡å‹å®šä¹‰


```python
class CNN(nn.Layer):
    def __init__(self, vocab, embedding_dim, n_filter, class_dim, 
                filter_size, num_channels, batch_size, seq_len):
        super(CNN, self).__init__()
        self.vocab_size = len(vocab)
        self.padding_idx = vocab['<unk>']
        self.embedding_dim = embedding_dim
        self.n_filter = n_filter
        self.class_dim = class_dim
        self.num_channels = num_channels
        self.filter_size = filter_size
        self.batch_size = batch_size
        self.seq_len = seq_len

        self.embedding = nn.Embedding(self.vocab_size, self.embedding_dim, 
                        sparse=False, padding_idx = self.padding_idx)
        self.conv = nn.Conv2D(in_channels=1,                        #é€šé“æ•°
                        out_channels=self.n_filter,                 #å·ç§¯æ ¸ä¸ªæ•°
                        kernel_size= [self.filter_size, self.embedding_dim],  #å·ç§¯æ ¸å¤§å°
                        padding=[1, 1]
                        )      
        self.relu = nn.ReLU()
        self.maxpool = nn.MaxPool2D(kernel_size = 2, stride = 2)
        self.fc = nn.Linear(int(self.n_filter*self.seq_len/2), 2)

    def forward(self, text):
        
        #print('è¾“å…¥ç»´åº¦ï¼š', input.shape)
        x = self.embedding(text)
        x_shape = [self.batch_size, self.num_channels, self.seq_len, self.embedding_dim]
        x = paddle.reshape(x, x_shape)   # [32, 1, 150, 128]
        x = self.relu(self.conv(x))
        #print('ç¬¬ä¸€å±‚å·ç§¯è¾“å‡ºç»´åº¦ï¼š', x.shape)
        x = self.maxpool(x)
        #print('æ± åŒ–åè¾“å‡ºç»´åº¦ï¼š', x.shape)
        #åœ¨è¾“å…¥å…¨è¿æ¥å±‚æ—¶ï¼Œéœ€å°†ç‰¹å¾å›¾æ‹‰å¹³ä¼šè‡ªåŠ¨å°†æ•°æ®æ‹‰å¹³.

        x = paddle.reshape(x, shape=[self.batch_size, -1])
        out = self.fc(x)
        return out                 

vocab = load_vocab(dict_path)
model = CNN(vocab, EMBEDDING_DIM, N_FILTER, CLASS_DIM, FILTER_SIZE, NUM_CHANNELS, BATCH_SIZE, SEQ_LEN)

```

## å››ã€æ¨¡å‹è®­ç»ƒ

å®šä¹‰å¥½æ¨¡å‹ä¹‹åï¼Œæˆ‘ä»¬ä¾¿å¼€å§‹è¿›è¡Œæ¨¡å‹è®­ç»ƒï¼Œä¸ºäº†ä¾¿æ·ï¼Œæ­¤å¤„ä»…å¯¹æ•°æ®è¿›è¡Œäº† 3 ä¸ª epochçš„è®­ç»ƒï¼Œå¹¶å°†æ¯ä¸ª batch_size ä¸‹çš„æŸå¤±å’Œç²¾ç¡®åº¦è¿›è¡Œäº†è®°å½•ï¼Œæ–¹ä¾¿åé¢è¿›è¡Œå¯è§†åŒ–åˆ†æã€‚


```python
import matplotlib.pyplot as plt

# è¿›è¡Œè®­ç»ƒ
def train(model, model_save_path):
    model.train() # è¿›å…¥è®­ç»ƒæ¨¡å¼

    # é€‰æ‹© Adam ä¼˜åŒ–å™¨
    opt = optimizer.Adam(learning_rate=LEARNING_RATE, parameters=model.parameters())
    steps = 0
    Iters, total_loss, total_acc = [], [], [] # è®°å½•æ¯ä¸ª iteration çš„æŸå¤±å€¼å’Œç²¾ç¡®åº¦
    for epoch in range(3):
        for batch_id, data in enumerate(train_loader):
            steps += 1
            sent = data[0]
            label = data[1].astype('float32') # å°† label å€¼è½¬åŒ–ä¸ºæµ®ç‚¹å‹ï¼Œä¸ç„¶åœ¨è®¡ç®— cross_entropy æ—¶ä¼šæŠ¥é”™
            logits = model(sent)
            # è®¡ç®—åˆ†ç±»äº¤å‰ç†µ
            loss = F.cross_entropy(logits, label, soft_label = True)
            # ä½¿ç”¨ argmax å¯¹å¾—åˆ°çš„æ¦‚ç‡å€¼è¿›è¡Œæ¨ç†ï¼Œå¾—åˆ°æœ€ç»ˆçš„æƒ…æ„Ÿæ ‡ç­¾
            real = paddle.argmax(label, axis = 1).unsqueeze(axis = 1)
            acc = metric.accuracy(logits, real) # è®¡ç®—ç²¾ç¡®åº¦
            if batch_id % 50 == 0:
                Iters.append(steps)
                total_loss.append(loss.numpy()[0])
                total_acc.append(acc.numpy()[0])
                print("epoch: {}, batch_id: {}, loss is: {}".format(epoch, batch_id, loss.numpy()))
            # åå‘ä¼ æ’­
            loss.backward()
            opt.step()
            opt.clear_grad()

        # evaluate model after one epoch
        model.eval() # è¯„ä¼°æ¨¡å¼
        accuracies = []
        losses = []
        for batch_id, data in enumerate(test_loader):
            sent = data[0]
            label = data[1].astype('float32') # åŒä¸Š
            logits = model(sent)
            loss = F.cross_entropy(logits, label, soft_label = True)
            
            real = paddle.argmax(label, axis = 1).unsqueeze(axis = 1)
            acc = metric.accuracy(logits, real)
            accuracies.append(acc.numpy())
            losses.append(loss.numpy())
        
        avg_acc, avg_loss = np.mean(accuracies), np.mean(losses)
        print("[validation] accuracy: {}, loss: {}".format(avg_acc, avg_loss))
        model.train()

    paddle.save(model.state_dict(),model_save_path)
    return Iters, total_loss, total_acc

model_save_path = export_path + 'model_final.bin'   # ä¿å­˜æ¨¡å‹
model = CNN(vocab, EMBEDDING_DIM, N_FILTER, CLASS_DIM, FILTER_SIZE, NUM_CHANNELS, BATCH_SIZE, SEQ_LEN)

iters, total_loss, total_acc = train(model, model_save_path)
```

    epoch: 0, batch_id: 0, loss is: [0.6942979]
    epoch: 0, batch_id: 50, loss is: [0.6843153]
    epoch: 0, batch_id: 100, loss is: [0.61038387]
    [validation] accuracy: 0.640625, loss: 0.6134560108184814
    epoch: 1, batch_id: 0, loss is: [0.3586772]
    epoch: 1, batch_id: 50, loss is: [0.38573164]
    epoch: 1, batch_id: 100, loss is: [0.27235347]
    [validation] accuracy: 0.7065972089767456, loss: 0.5871999859809875
    epoch: 2, batch_id: 0, loss is: [0.08663776]
    epoch: 2, batch_id: 50, loss is: [0.05753569]
    epoch: 2, batch_id: 100, loss is: [0.03202415]
    [validation] accuracy: 0.7048611044883728, loss: 0.717033326625824


- ç»˜åˆ¶æŸå¤±å€¼å’Œç²¾ç¡®åº¦çš„å˜åŒ–è¶‹åŠ¿å›¾


```python
# ç»˜å›¾
def draw_process(title,color,iters,data,label):
    plt.title(title, fontsize=24)
    plt.xlabel("iter", fontsize=20)
    plt.ylabel(label, fontsize=20)
    plt.plot(iters, data,color=color,label=label) 
    plt.legend()
    plt.grid()
    plt.show()
```


```python
draw_process("trainning loss","red",iters,total_loss,"trainning loss")
```


    
![png](./Img/output_32_0.png)
    



```python
draw_process("trainning acc","green",iters,total_acc,"trainning acc")
```


    
![png](./Img/output_33_0.png)
    


## äº”ã€æ¨¡å‹è¯„ä¼°

æ¥ä¸‹æ¥ï¼Œç”¨äº‹å…ˆåˆ’åˆ†å¥½çš„éªŒè¯é›†æ¥å¯¹è®­ç»ƒå¥½çš„æ¨¡å‹è¿›è¡Œè¯„ä¼°ã€‚äº‹å®ä¸Šï¼Œæˆ‘ä»¬è¿˜å¯ä»¥ä½¿ç”¨å…¶ä»–é¢†åŸŸçš„äº§å“è¯„è®ºæ•°æ®é›†æ¥å¯¹æˆ‘ä»¬çš„æ¨¡å‹è¿›è¡Œè¯„ä¼°ï¼ˆè€ƒè™‘åˆ°ä¸å·²ç»å±•ç¤ºçš„å†…å®¹å¹¶æ²¡æœ‰å¤ªå¤§çš„åˆ›æ–°ç‚¹ï¼Œä»…éœ€è¦æŠŠå‰é¢çš„ä»£ç åœ¨è¿è¡Œä¸€éï¼Œå› æ­¤æ­¤å¤„ä¸å†è¿›è¡Œèµ˜è¿°ï¼‰ã€‚


```python
model_state_dict = paddle.load(model_save_path)
model = CNN(vocab, EMBEDDING_DIM, N_FILTER, CLASS_DIM, FILTER_SIZE, NUM_CHANNELS, BATCH_SIZE, SEQ_LEN)

model.set_state_dict(model_state_dict) 
model.eval()
accuracies = []
losses = []
Iters = []
steps = 0

for batch_id, data in enumerate(test_loader):
    steps += 1
    sent = data[0]
    label = data[1].astype('float32')
    logits = model(sent)
    loss = F.cross_entropy(logits, label, soft_label = True)
    
    real = paddle.argmax(label, axis = 1).unsqueeze(axis = 1)
    acc = metric.accuracy(logits, real)
    
    Iters.append(steps)
    accuracies.append(acc.numpy())
    losses.append(loss.numpy())

avg_acc, avg_loss = np.mean(accuracies), np.mean(losses)
print("[validation] accuracy: {}, loss: {}".format(avg_acc, avg_loss))

```

    [validation] accuracy: 0.7048611044883728, loss: 0.7096621990203857



```python
draw_process("trainning loss","red",Iters,losses,"trainning loss")
```


    
![png](./Img/output_36_0.png)
    



```python
draw_process("trainning acc","green",iters,total_acc,"trainning acc")
```


    
![png](./Img/output_37_0.png)
    


**ç»“è®º**ï¼šå¯ä»¥å‘ç°ï¼Œæ€»ä½“è€Œè¨€ï¼Œæ¨¡å‹è¿˜æ˜¯å¾—åˆ°äº†ä¸é”™çš„æ³›åŒ–æ•ˆæœï¼Œå¹³å‡ç²¾ç¡®åº¦èƒ½åˆ°åˆ° 70%ï¼Œäº‹å®ä¸Šï¼Œè¿˜å¯ä»¥é€šè¿‡è°ƒæ•´æ¨¡å‹çš„å­¦ä¹ ç‡ï¼Œbatch_size, optimizer, filter_size, n_filtersç­‰å‚æ•°ï¼Œæ¥è°ƒè¯•æ¨¡å‹ï¼Œä»¥è·å¾—æ›´é«˜çš„ç²¾åº¦ï¼Œå‡ºäºæ—¶é—´è€ƒè™‘ï¼Œæ­¤å¤„ä¸å†è¿›è¡Œå±•å¼€ã€‚

# å…­ã€æ€»ç»“ä¸å‡å

æœ¬é¡¹ç›®å€ŸåŠ© `CNN` å·ç§¯ç¥ç»ç½‘ç»œæ¨¡å‹ï¼Œå¯¹ä¹¦æœ¬äº§å“è¯„è®ºè¿›è¡Œäº†ç®€å•çš„æƒ…æ„Ÿåˆ†æï¼Œæœ¬é¡¹ç›®æ‰€ä½¿ç”¨çš„æ•°æ®é›†ä¸ºï¼šMulti-Domain Sentiment Dataset æ•°æ®é›†ä¸­çš„ `book` æ•°æ®é›†ã€‚

## ä¼˜ç‚¹å’Œåˆ›æ–°ç‚¹

æœ¬é¡¹ç›®å®ç°äº†ä¸€ä¸ªå®Œæ•´çš„è½»é‡å‹æ–‡æœ¬æƒ…æ„Ÿåˆ†ææ¨¡å‹ï¼Œå€ŸåŠ© `nltk` å·¥å…·ï¼Œæœ¬é¡¹ç›®è‡ªå·±å®šä¹‰äº†å•è¯è¡¨å’Œå­—å…¸ï¼Œä¸åŒäºä»¥å¾€çš„åŸºäºè¯å…¸çš„æƒ…æ„Ÿåˆ†æï¼Œæœ¬é¡¹ç›®ä½¿ç”¨äº†è¯åµŒå…¥çš„å½¢å¼è¿›è¡Œæ–‡æœ¬åˆ†æã€‚è¿™æ ·å¤„ç†ä½¿å¾—é¡¹ç›®å¯¹å•è¯å…·æœ‰è¾ƒé«˜çš„å®¹é”™æ€§ã€‚ç‰¹åˆ«çš„ï¼Œæœ¬é¡¹ç›®å¯¹å•è¯è¿›è¡Œäº†è¯å¹²æå–ï¼Œè¿™æ ·ä½¿å¾—åœ¨è¿›è¡Œåˆ†æçš„æ—¶å€™ï¼Œæƒ…æ„ŸæŒ‡æ ‡ä¸ä¼šéšç€è¯æ€§å‘ç”Ÿè¾ƒå¤§çš„æ”¹å˜ã€‚

## ç¼ºç‚¹å’Œå±•æœ›

1. æ•°æ®é›†å•ä¸€ã€‚å‡ºäºæ—¶é—´çš„è€ƒé‡ï¼Œæœ¬é¡¹ç›®ä»…ä½¿ç”¨äº†æ•°æ®é›†ä¸­çš„ `books-review` æ•°æ®ï¼Œå› æ­¤å¯èƒ½ç»“è®ºä¸Šä¸å…·å¤‡éå¸¸å¼ºçš„è¯´æœåŠ›ã€‚ä½†æ˜¯æ¨¡å‹çš„æ•´ä½“æ¡†æ¶æ˜¯å®Œæ•´çš„ï¼Œæ—¶é—´å……è¶³çš„æƒ…å†µä¸‹ï¼Œ å¯ä»¥åœ¨å¯¹æ•°æ®é›†è¿›è¡Œæ‰©å……ï¼Œæ„å»ºæ›´ä¸ºå®Œå¤‡çš„æ•°æ®é›†æ¥è®­ç»ƒæˆ‘ä»¬çš„æ¨¡å‹ã€‚

2. æ¨¡å‹è¾ƒä¸ºç®€å•ã€‚å‡ºäºç®€å•å’Œå¯é˜…è¯»æ€§ï¼Œæœ¬æ–‡è®¾ç½®çš„æ¨¡å‹å’Œå‚æ•°éƒ½è¾ƒä¸ºç®€å•ã€‚æ¯”å¦‚åœ¨è®¾ç½® `filter_size` æ—¶ä»…ä»…ä½¿ç”¨çš„ `3`ï¼Œäº‹å®ä¸Šï¼Œæˆ‘ä»¬å¯ä»¥è®¾ç½®ä¸€äº›åˆ—å€¼ `[2, ,3, 4]` æ¥æå‡æ¨¡å‹å¯¹äºè¯ç»„çš„å…³æ³¨åº¦ã€‚è¿™åœ¨åç»­æ·±å…¥æ¢ç©¶æˆ–æ¨¡å‹æå‡æ—¶å¯ä»¥çº³å…¥è€ƒé‡ã€‚

3. è¯åµŒå…¥è®¾ç½®ç®€å•ã€‚æœ¬é¡¹ç›®åœ¨å¯¹æ–‡æœ¬è¿›è¡Œå¤„ç†ä¸­ä½¿ç”¨äº† `embedding` æŠ€æœ¯ï¼Œè€ƒè™‘åˆ°å¹³å°å®¹é‡ï¼Œå¹¶æ²¡æœ‰ä½¿ç”¨é¢„è®­ç»ƒçš„ `embedding` å‘é‡ï¼Œäº‹å®ä¸Šï¼Œæœ‰ç ”ç©¶è¡¨æ˜ï¼Œ å¦‚æœä½¿ç”¨æ–¯å¦ç¦æå‡ºçš„ `Glove` é¢„è®­ç»ƒçš„å‘é‡ï¼Œé€šå¸¸éƒ½èƒ½å¾—åˆ°æ›´ä¸ºä»¤äººæ»¡æ„çš„é¢„æµ‹æ•ˆæœã€‚å› æ­¤åœ¨å®é™…å·¥ç¨‹éƒ¨ç½²è¿‡ç¨‹ä¸­ï¼Œå¯ä»¥ä½¿ç”¨å·²ç»è®­ç»ƒå¥½çš„å‘é‡ä½œä¸ºè¯åµŒå…¥å‘é‡ã€‚

4. æ¨¡å‹åˆ›æ–°æ€§æ¬ ç¼ºã€‚äº‹å®ä¸Šï¼Œç›®å‰è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸæœ‰è®¸å¤šå‰æ²¿çš„æ¨¡å‹å¯ä»¥ç”¨äºå¤„ç†æ–‡æœ¬æƒ…æ„Ÿï¼Œå¦‚ `bert` ç­‰ï¼Œè€Œæ­¤å¤„ä½¿ç”¨çš„æ¨¡å‹è¾ƒä¸ºç®€å•ï¼Œä¹Ÿå­˜åœ¨è®¸å¤šå›ºæœ‰çš„å±€é™ï¼Œå¦‚æ²¡æ³•åŒºåˆ†ä¸€è¯å¤šä¹‰çš„ç°è±¡ç­‰ã€‚å› æ­¤åç»­å¯ä»¥è€ƒè™‘ä½¿ç”¨æ›´å¤æ‚ã€æ›´å…¨é¢çš„æ¨¡å‹è¿›è¡Œæƒ…æ„Ÿåˆ†æã€‚

# ä¸ƒã€ä¸ªäººæ€»ç»“

## ä½œè€…ç®€ä»‹
ç¬”å YangSuï¼ˆä»°æ¶‘ï¼‰ï¼Œå¦é—¨å¤§å­¦ç®¡ç†å­¦é™¢åœ¨è¯»ç ”ç©¶ç”Ÿï¼Œç ”ç©¶æ–¹å‘æ˜¯é‡‘èä¸äººå·¥æ™ºèƒ½çš„äº¤å‰ï¼Œä¸šä½™çˆ±å¥½æ˜¯é˜…è¯»å’Œå¥èº«ï¼Œå¹³å¸¸ä¼šè‡ªä¸»å­¦ä¹ ä¸€äº›äººå·¥æ™ºèƒ½é¢†åŸŸå†…çš„å‰æ²¿çŸ¥è¯†ï¼Œç›®å‰åˆšå…´è¶£çš„æ–¹å‘æ˜¯è‡ªç„¶è¯­è¨€å¤„ç†ï¼Œä¸ªäººä¸»é¡µï¼š[ç‹¬å­¤è¯—äººçš„å­¦ä¹ é©¿ç«™](yangsuoly.com)

## æäº¤é“¾æ¥
aistudioé“¾æ¥ï¼š[æ–‡æœ¬æƒ…æ„Ÿåˆ†ç±»ï¼ˆå®Œæ•´ä»£ç ï¼‰](https://aistudio.baidu.com/aistudio/projectdetail/3488745?contributionType=1)

githubé“¾æ¥ï¼š[æ–‡æœ¬æƒ…æ„Ÿåˆ†æï¼ˆåŸºäºé£æµ†)](https://github.com/YangSuoly/Sentiment-analysis)
ï¼ˆå»ºè®®ä½¿ç”¨ç¬¬ä¸€ä¸ªé“¾æ¥forké¡¹ç›®å¹¶è¿è¡Œï¼‰

giteeé“¾æ¥ï¼š[æ–‡æœ¬æƒ…æ„Ÿåˆ†æï¼ˆåŸºäºé£æµ†)](https://gitee.com/yangsuoly/sentiment-analysis)
ï¼ˆå»ºè®®ä½¿ç”¨ç¬¬ä¸€ä¸ªé“¾æ¥forké¡¹ç›®å¹¶è¿è¡Œï¼‰
